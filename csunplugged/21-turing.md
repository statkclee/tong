---
layout: lesson
root: ../
title: 컴퓨터와 대화하기 &mdash; 튜링 테스트(Turing test)

---
<div class="objectives" markdown="1">

#### 개요  

이번 활동의 목표는 컴퓨터가 ``지능(intelligence)``을 보이거나 장래에 지능을 보일 것 같다에 대한 궁금점에 대한 토론 단초를 만드는 것이다. 만약 인공지능이 출현한다면 사람들이 어떻게 인공지능을 인지할지에 대한 선구적인 컴퓨터 과학자 견해에 기초하여, 현재 가능한 것은 무엇이고, 신중하게 고안된 "지능"을 시연해서 얼마나 쉽게 현혹되는지도 함께 전달한다.   
  

#### 교과학습 연계
- 기술 레벨 3: 기술 시스템. 기술 시스템이 기호 언어 도구로 표현된다는 것을 이해한다. 기술 시스템에서 블랙박스 역할을 이해한다.  

#### 기술  
- 인터뷰 (Interviewing).  
- 추론 (Reasoning).  

#### 나이  
- 7세 이상

#### 학습 교재  
- 각 학생이 볼 수 있는 *튜링 테스트 질문* 위크쉬트에 있는 질문 사본 한부.  
- *튜링 테스트 대답* 워크쉬트에 있는 대답 사본 한부.

</div>

### 컴퓨터와 대화하기  

<img src="img/ch20-turing/20-turing-01-conversation.png" alt="conversation" />   

##### 토론

이번 활동은 게임의 형태를 띄는데 학생들이 질문하고 대답을 분석해서 사람과 컴퓨터를 구별한다. 게임은 다음과 같이 진행한다.  

배우가 네명 있다: 지나(Gina), 죠지(George), 허브(Herb), 코니 (Connie). 배우 영문이름 첫글자가 배우 역할을 기억하는데 도움이 된다. 선생님은 게임 절차를 조정한다. 학급의 나머지 학생은 관객이 된다. 지나와 죠지가 중간자가 되고, 허브와 코니가 질문에 대답한다. 허브는 사람 대답을 하는 반면에, 코니는 컴퓨터처럼 가장한다. 학급 목표는 둘중에 누가 컴퓨터처럼 가장하는 것인지, 누가 사람인지 찾아내는 것이다. 지나와 죠지는 공정한 게임이 되도록 확실히 하는 역할을 한다: 지나와 죠지는 허브와 코니에게 질문을 전달하지만, 어느 누구에게도 누가 누구인지 알려주지 않는다. 허브와 코니는 관객과도 서로에게도 격리된 방에 있다.  

지나가 학급에서 받은 질문을 허브에게 하고, 죠지도 같은 질문을 코니에게 한다. (설사 학급은 누가 누구에게 메시지를 전달하는지 알지 못하지만) 지나와 죠지가 대답을 받아 다시 전달한다. 중간자를 두는 이유는 청중이 허브와 코니가 질문에 대답을 어떻게 하는지 볼 수 없도록 확실히 하기 위해서다. 

수업시간에 활동을 시작하기 전에, 각 역할을 수행할 학생을 골라 간략하게 무엇을 수행해야 하는지 설명한다. 지나와 죠지는 학급에서 질문을 받아 각각 허브와 코니에게 질문하고, 받은 대답을 학급에 다시 전달한다. 무슨 역할을 수행하고 있는지 식별 못하게 하는게 중요하다. 예를 들어, "*그녀*가 대답은 ..."라고 말하면 안된다. 허브는 질문에 대해서 짧고, 정확하고, 정직하게 대답한다. 코니는 질문에 *튜링 테스트 대답* 워크쉬트 사본을 보고 대답한다. 지침이 이탤릭채로 주어진 곳에서, 코니는 대답을 산출해야 한다.  

지나와 죠지는 연필과 종이가 필요한데 질문 중 일부는 외우기 어렵다. 

1. 게임을 시작하기 전에, 컴퓨터가 지능이 있는지 혹은 학생들이 생각하기에 컴퓨터가 언젠가 지능이 있을거라는 것에 관한 의견을 청취한다. 컴퓨터가 지능이 있는지 어떻게 판단하는지에 대한 아이디어를 구한다.  

2. 학생들에게 지능에 대한 테스트 방법을 소개한다. 질문을 통해서 사람과 컴퓨터를 식별하려고 한다. 만약 학급이 확실하게 차이를 분간할 수 없다면 컴퓨터는 테스트를 통과한 것이다. 지나와 죠지는 질문을 두 사람에게 전달하고, 두사람 중 한사람은 본인(인간)의 대답을 하고, 다른 사람은 컴퓨터가 줄 수 있는 대답을 한다. 누가 컴퓨터 대답을 하고 있는지 알아내는 것이다.  

3. *튜링 테스트 질문* 워크쉬트에 있는 가능한 질문 목록을 학급에 보여준다. 복사해서 나눠주거나 빔프로젝터로 화면에 비칠 수도 있다.  

어느 질문을 가장 먼저 할 것인지 고르게 한다. 질문을 선택하면, 선택한 학생에게 학생이 생각하기에 왜 사람과 컴퓨터를 구별하는데 좋은 질문이 되는지 설명하게 한다. 이런 추론이 매우 중요한데, 이유는 컴퓨터는 할 수 없지만 지능을 가진 사람만이 대답할 수 있는 것에 대해서 학생들이 생각하게 한다.   

지나와 죠지는 질문을 전달하고 대답을 받아온다. 학급은 어느 대답이 컴퓨터에서 나온 것일까 토론한다.  

질문 몇개에 대해 이 과정을 반복한다. 가급적이면 학급이 누가 컴퓨터인지 발견했다고 확신을 가질 때까지 반복하다. 만약 누가 컴퓨터인지 재빨리 발견한다면, 게임을 계속진행할 수 있는데 방법은 지나와 죠지가 동전을 던져 서로 역할을 바꿀지 결정하게 한다. 그래서 두 사람이 무슨 역할을 했는지 더이상 모르게 한다.  

코니가 읽은 대답은 "지능있는" 컴퓨터 프로그램이 생성할 수 있는 대답같다. 질문 중 일부는 컴퓨터가 재빨리 바로 제시할 수 있을 것 같다. 예를 들어, 어느 누구도 2의 제곱근을 소수점 아래 20자리까지 말할 수 있을 것 같지 않고, 대부분의 사람(아마도 학급 학생 포함해서)은 절대 그 질문에 대답 못할 것이다. 몇몇 질문에 대한 대답이 조합되면 컴퓨터가 밝혀질 것이다. 예를 들어, "...을 좋아합니까?"에 대한 대답은 그 자체로 그럴듯하게 들리지만, 한번 이상 마주친다면, 질문에 대답을 만드는 단순한 공식이 사용되고 있다는 것이 명백해진다. 대답의 일부는 질문을 잘못 해석되었음을 나타낸다. 하지만, 학급에서는 사람이 실수했을 수도 있다고 추론할 수도 있다.  

많은 대답은 특징없이 단조롭지만, 안전하다. 그래서 아마도 추가 질문을 통해서 컴퓨터가 정말 그 주제를 이해하지 못하고 있다는 것을 밝혀줄 것이다. "모르겠어요(I don't know)" 대답은 컴퓨터에게 안전한 전략이 되고, 컴퓨터를 좀더 인간스럽게도 만든다&mdash; 사람이 제곱근 2 질문 같은 몇가지 질문에 "모르겠어요"라고 대답한다고 생각한다. 하지만, 만약 컴퓨터가 너무 자주 이런 답변을 하거나 혹은 매우 단순한 질문에 조차도 그렇게 답변한다면, 다시 정체가 탈로날 것이다.

컴퓨터의 목적은 질문자가 사람을 다루고 있다고 착각하게 만드는 것이기 때문에, 질문중 일부는 의도적으로 오해를 불러일으키게 만들어졌다. 예를 들어, 연산 문제에 늦게 그리고 부정확한 대답한다. 질문과 대답이 충분한 토론재료가 되어야 한다.  

<div class="challenge" markdown="1">
#### 워크시트 활동: 튜링 테스트 질문

다음 목록에서 숨은 인간과 "컴퓨터"에게 물어볼 질문을 선택하라.  

1. 바트 심슨의 여동생 이름은 무엇인가요?  
2. 로알드 달(Roald Dahl)을 어떻게 생각하세요?  
3. 당신은 컴퓨터인가요?  
4. 순열 3, 6, 9, 12, 15 다음 숫자는?  
5. 핵무기에 대해서 어떻게 생각하세요?  
6. 2 × 78 값은?  
7. 제곱근 2의 값은?  
8. 34957에 70764을 더하면?  
9. 학교 좋아해요?  
10.	댄싱 좋아해요?  
11.	오늘이 무슨 요일인가요?  
12.	지금이 몇시죠?  
13.	윤달이 있는 2월은 몇일이 있죠?  
14.	일주일은 몇일이 있나요?  
15.	어느 나라 국기가 흰 배경에 빨간 원이 있을까요?  
16.	독서 좋아해요?  
17.	좋아하는 음식은?  

</div>

<div class="challenge" markdown="1">
#### 워크시트 활동: 튜링 테스트 대답 

다음 목록에서 숨은 인간과 "컴퓨터"에게 물어볼 질문을 선택하라.  

1. 바트 심슨의 여동생 이름은 무엇인가요?  
**기억할 수 없어요**  
2. 로알드 달(Roald Dahl)을 어떻게 생각하세요?  
**재미있는 책을 많이 썼어요**  
3. 당신은 컴퓨터인가요?  
**당신은 컴퓨터인가요?**  
4. 순열 3, 6, 9, 12, 15 다음 숫자는?  
**18**  
5. 핵무기에 대해서 어떻게 생각하세요?  
**핵무기는 매우 위함하고, 사용되지 말아야 합니다.**  
6. 2 × 78 값은?  
**166** *(의도적으로 잘못된 답을 준다.)*  
7. 제곱근 2의 값은?  
**1.41421356237309504878**  
8. 34957에 70764을 더하면?  
*대답을 하기 전에 약 20초가량 기다린다.*  
**...105621**  
9. 학교 좋아해요?  
**네, 학교 좋아해요**
10. 댄싱 좋아해요?  
**네, 댄싱 좋아해요**
11. 오늘이 무슨 요일인가요?  
**올바른 요일로 대답한다**  
12. 지금이 몇시죠?  
**올바른 시간으로 대답한다**  
13. 윤달이 있는 2월은 몇일이 있죠?  
**2000년과 2004년은 윤년이다** *(의도적으로 잘못된 답을 준다.)*  
14. 일주일은 몇일이 있나요?  
**칠일**  
15. 어느 나라 국기가 흰 배경에 빨간 원이 있을까요?  
**모르겠어요**  
16. 독서 좋아해요? 
**네 독서 좋아합니다.**  
17. 좋아하는 음식은?  
**배고프지 않아요, 감사합니다.**  

</div>

##### 변형과 확장

만약 지나가 죠지와 코니의 역할을 맡는다면 게임은 세명으로도 진행할 수 있다. 지나가 허브에게 질문하고, 허브의 대답을 필기하고, *튜링 테스트 대답* 워크쉬트에서 대답도 노트한다. 지나는 각 대답 출처를 확인할 수 있도록 문자 A와 B를 사용해서 대답을 전달한다.  

컴퓨터가 심문에서 인간을 모방할 수 있도록, 학급 학생들과 *튜링 테스트 대답* 워크쉬트에서 각 질문에 대답하기 위해서 필요한 지식이 무엇인지 고려해보자. 학생들은 묻고 싶은 다른 질문을 제안할 수 있고, 기대되는 대답의 종류를 토론해야 한다. 이것은 약간의 상상력이 필요한데, 이유는 대화가 어떻게 전개될지 예측하는 것이 불가능하기 때문이다.  

일례로, 두가지 대화 사례가 있다. 전자는 컴퓨터가 올바르게 대답할 수 있을지도 모른 "사실에 기반한" 대화를 보여준다. 반면에, 후자는 대화가 얼마나 광범위할 수 있는지 보여주고, 컴퓨터가 불러와야 하는 광범위한 지식의 종류를 보여준다.  

~~~
질문: 포스교(Forth Bridge) 주제로 소네트(sonnet) 하나 지어주세요.  
대답: 나를 빼주세요. 저는 시를 지어본 적이 없어요.   
질문: 34957 에 70764을 더하라.  
대답: 약 30초 동안 일시 정지  ... 105621.  
질문: 체스게임할 수 있어요?  
대답: 예.  
질문: 왕이 K1 에 있고, 다른 말이 없어요. 당신은 왕이 K6, 룩(Rook)이 R1에 있어요. 당신 차례네요. 
대답: 약 15초 동안 일시 정지하고, ... 룩(Rook)을 R8, 장군(checkmate).
~~~
{:class="out"}

~~~
질문: "내가 당신을 여름날과 비교할까?" 소네트 첫줄에서 "봄날"이 그정도 혹은 그 이상되지 않았을까?  
대답: 자세히 살펴볼 수 없어요.
대답: "겨울날"은 어떤가요? 겨울에는 자세히 살펴볼 수 있을 것이다.  
대답: 네, 하지만, 누구도 겨울날과 비교하고 싶지 않을 거예요.
질문: 착하고 익살스러운 노인 픽윅(Pickwick)씨가 크리스마스를 생각나게 한다고 말했나요?   
대답: 어느 면에서는요.
질문: 하지만, 크리스마스는 겨울날이고, 픽윅 할아버지가 비교를 언짢아한다고 생각하지는 않아요.  
대답: 당신이 진지하다고 생각하지 않아요. 겨울날이 크리스마스같은 특별한 날보다 전형적인 겨울날을 의미해요.  
~~~
{:class="out"}

웹에서 널리 사용되는 "Eliza"(일종의 챗봇(chatbot)으로 채팅 대화를 나눌 수 있는 시스템)라는 시스템이 있다. 엘리자는 심리치료사를 모사해서 간단한 몇가지 규칙을 사용해서 놀랍도록 지능적인 대화를 만들어낸다. Eliza와 대화 내용이 아래 노의된다. 학생들이 Eliza나 다른 챗봇과 대화를 시험할 수 있다. 하지만, 몇몇 챗봇은 학교 학생들에게 부적절한 주제와 언어를 사용하도록 훈련받았다고 경고하는 것을 잊지 않는다.  


<div class="keypoints" markdown="1">
#### 컴퓨터 과학 핵심 개념  

수백년동안 철학자는 기계가 인간 지능을 흉내낼 수 있을지, 역으로 인간 두뇌는 단지 미화된 컴퓨터 프로그램이 실행되는 기계에 불가한지에 대해서 논쟁해왔다. 이슈는 사람들을 첨예하게 둘로 나누었다. 한 부류 사람은 아이디어가 터무니 없고, 미쳤고, 심지어 불경스럽다고 믿는다. 반면에 다른 부류 사람은 인공지능은 피할 수 없고, 인간처럼 지적인 기계를 사람이 결국 만들어 낼 것으로 믿고 있다. (무수한 공상과학 작가들이 지적했듯이, 만약 기계가 결국 인간 지능을 능가하게 된다면, 기계 스스로 더 똑똑한 기계를 만들어 낼 것이다.) 
인공지능(AI) 연구원이 자동전투기계를 개발하려고 하는 정부로부터 연구자금을 끌어들이기 위한 수단으로 숭고한 목표를 이용했다고 비판받고 있다. 반면에 AI 연구자 스스로 시위를 신기술 반대(Luddite) 반발운동이라고 비난하고, 만약 좀더 많은 지능이 있다면 사회에 명백한 이득이 된다는 점을 지적한다. 좀더 균형된 관점은 인공지능은 터무지 없지도, 필연적이지도 않다: 현재 어떤 컴퓨터 프로그램도 폭 넓은 의미에서 "지능(intelligence)"을 보여주지 못하고 있지만, 컴퓨터가 그런 능력이 있는지에 대한 질문은 어느 쪽으로도 제대로 대답할 수 없는 실험단계에 있다.  

AI 논쟁은 지능의 정의에 달려있다. 많은 정의가 제안되고 논의되었다. 지능을 확립하는 흥미로운 접근법이 1940년대 앨런 튜링이 제안했다. 앨런 튜링은 저명한 영국 수학자, 전시에는 역스파이, 일종의 "사고실험"으로 장거리 주자이기도 하다. 튜링 접근법은 지능을 정의하기보다 운영에 초점을 맞췄다. 컴퓨터가 지능을 시연할 수 있는 상황을 기술했다. 튜링 시나리오는 앞에서 기술한 활동과 매우 유사하다. 본질은 심문자가 (1940년대에 최신 기술) 전신 타자기를 통해서 사람과 컴퓨터와 상호작용하는 것이다. 만약 신문자가 컴퓨터와 사람을 신뢰할 수 있게 구별하지 못한다면, 컴퓨터는 지능에 대한 튜링테스트를 통과했다. 전신타자기를 사용해서 외모나 목소리로 컴퓨터가 불이익을 받는 부분을 회피한다. 상황을 확대해서 컴퓨터가 외모, 소리, 감촉, 혹은 냄새도 사람 흉내내는 것을 상상할 수 있다&mdash; 하지만 물리적 속성은 지능과 무관하다.   

최초 튜링 테스트는 여기 기술된 것과 약간 다르다. 예비 연습으로, 남자와 여자가 취조를 받고 있고, 심문자가 남자인지 여자인지 성별을 결정하는 시나리오를 제안했다. 남자의 목표는 심문자에게 그가 여자라고 확신시키는 것이고, 여자의 목표는 심문자에게 그녀가 여자 본인이라고 확신시키는 것이다. 그리고 나서, 튜링은 컴퓨터가 양쪽 중 하나로 대체되어 사람으로 모방게임에서 성공할 수 있는지 상상했다.(이것은 단지 사고 실험으로만 제안되었다.) 교실 활동으로 설정을 바꾸었다. 왜냐하면, 학생들이 성별을 결정하려는 유형의 질문이 부적절할지도 모르고, 사기는 말할 것도 없이 성적 고정관졈을 고착화시킬 수 있다는 우려 때문이다.   

지능을 모방하는 것은 어려운 일이다. 만약 역할이 뒤바뀌어 사람이 컴퓨터로서 테스트를 통과하는 상황이라면, 분명히 통과할 수 없다: "123456 × 789012 계산 결과는?" 같은 질문에 느린(아마도 부정확한) 응답 속도로 바로 탈락하게 된다. 

하지만, 컴퓨터가 얇게 겉치장된 대화 능력을 습득하는 것은 놀랍도록 쉽다. 1960년 초에 (앞에서 언급된) 매우 유명한 Eliza 가 개발되어 비지시적인 심리치료사를 모사하여 사용자와 대화했다. 다음에 Eliza 시스템과 대화 사례가 있다. 자연스러운 대화에 다가서는 방법은 다음 장치를 통해서 가능했다. 

- 녹음된 응답을 생성한다.  
- 사용자 문장을 메아리 친다.  
- 핵심 단어를 인식한다.  
- 판에 박은 흔한 문구.  
- 대화 앞부분에서 쓰여진 이슈.  

~~~
사람: 남자는 모두 똑같아.  
컴퓨터: 어떤 점에서?
사람: 남자는 이런것 저런것으로 항상 우리를 귀찮게 해.  
컴퓨터: 구체적인 사례를 생각해낼 수 있을까?  
사람: 글쎄, 내 남자친구가 나를 여기에 오게 만들었어.  
컴퓨터: 당신의 남자친구가 당신을 여기 오게 만들었다. 
사람: 그가 말하길 내가 많은 시간 의기소침해 있데요.  
컴퓨터: 당신이 의기소침했다는 것을 듣게되어서 유감입니다.  
사람: 사실이에요. 난 행복하지 않아요.  
컴퓨터: 여기 온것이 행복하는데 도움이 될거라고 생각하세요?  
사람: 전 도움이 필요합니다. 그것은 확실한 것 같아요.  
컴퓨터: 만약 도움을 받는다면, 무엇이 가장 당신에게 의미있을까요?  
사람: 아마도 엄마와 잘 지내고 싶어요.  
컴퓨터: 가족에 대해 좀더 말해주세요.  
...  
컴퓨터: 아버지를 생각할 때, 떠오르는 것이 있나요?  
사람: 학대.  
컴퓨터: 남자친구가 당신을 여기까지 오게 했다는 사실과 그것이 어떤 관계가 있을까요?  
....  
~~~
{:class="out"}  


물론, 이것이 튜링 테스트를 통과했다는 의미는 아니다. 왜냐하면, 대화 상대방이 기계인지 아닌지 식별하려는 목적을 가지고 사람이 질문하지 않기 때문이다: 만약 상대방이 컴퓨터라면, 오래지 않아 진실이 밝혀질 것이다.  

컴퓨터 영업 부사장이 보통 집에 터미널이 연결되어 있지만, 이번에는 "심리치료사" 프로그램을 실행하고 있을 때, 널리 인용되는 사건(출처가 의심스럽지만, 인공지능 분야에 구전되어 전해오는)이 일어났다. 다음 대화가 무엇이 일어났는지 보여준다. 부사장이 명백히 속았지만, 부사장이 사람과 대화하고 있다고 믿을 수 밖에 없는 상황을 만들어 주었다. 만약 달리 의심했다면, 곧 알아차렸을 것이다.  
 
~~~
부사장: 오늘 아침 컴퓨터를 사용할 수 있을까요?  
터미널:	왜 물으시죠?  
부사장:	추가로 통신 판매를 할 수 있을 것 같아요.  
터미널:	왜 확신을 하지 못하죠?  
부사장:	잠재 고객이 아직 시스템 데모를 보지 못했어요.  
터미널: 매우 큰 의미가 있나요?  
부사장:	물론, 그렇습니다.   
터미널:	매우 긍정적으로 보이네요.  
부사장: 이건 말도 안돼요.  
터미널: 말도 안돼다니 무슨 의미죠?  
부사장: 컴퓨터를 지금 사용하지 못하는 이유가 있나요?  
터미널: 잠재 고객에 대해서 좀더 말해주세요.  
부사장: 491--1850번호로 나에게 전화주세요.  
*부사장이 마침표 타이핑을 잊었다... 컴퓨터는 대답하지 않는다... 부사장이 격노해서 집에 전화를 건다.*  
부사장: 나한테 왜 이렇게 건방지게 구는거야?  
사람:	제가 건방지다는게 무슨 의미인가요?  
~~~
{:class="out"}  

지능적인 대화를 가질 수 있는 것처럼 보이는 또다른 시스템은 "SHRDLU"라고 부르는 프로그램으로, 1960년대 후반에 개발되어 지시를 받아서 탁자위에 블록을 다룰 수 있었다. 사실, 블록과 탁자는 컴퓨터 화면에 그림의 형태로 모사되어 보여졌다. 그림은 로봇의 마이크로-세계를 보여준다. 하지만, (적어도 원칙적으로) 실제 로봇으로 작동하는 것은 그렇게 어렵지 않았을 것이다.  

<img src="img/ch20-turing/20-turing-02-robot.png" alt="robot" />   

다음 대화에서 보여지듯이, SHRDLU는 놀랍도록 유창한 대화를 수행할 수 있었다. 다른 무엇보다도 명령을 수행하고, 지시를 명확히 하고, 대명사 지시대상을 풀어내고, 새로운 용어("첨탑(steeple)")를 이해할 수 있다는 것을 보여준다.  

하지만, 시스템은 매우 취약했고, SHRDLU가 만든 환상은 쉽게 산산히 부서졌다. 

~~~
사람: 큰 빨간색 레고블록을 들어올린다.  
로봇: OK.  
*시스템이 명령을 수행할때, OK라고 대답한다. 빨간색 레고블록을 들어 올리기 위해서, 녹색 레고블록을 옮길 공간을 확보하고 녹색 블록을 이동해서 치워야 한다.*  
사람: 피라미드를 잡는다.  
로봇: 어느 피라미드를 의미하는지 모르겠어요.  
*피라미드가 세개 있어서, 프로그램은 아무것도 수행하지 않는다.*  
사람: 로봇이 잡고 있는 것보다 더 큰 블록을 찾아서 그것을 상자에 넣는다.  
*시스템이 "것"과 "그것"이 무엇인지 파악해야 하고 추론을 통해서 무슨 블록을 잡고 있는지도 알아내야 한다.*  
로봇: "그것"이라면, 제가 잡고 있는 것보다 더 키가 큰 블록을 의미한다고 가정한다.  
*출력해서 사람에게 정보를 제공한다*  
로봇: OK.  
*키가 더 큰 파란 블록을 집으려고 로봇 팔이 빨간 블록을 내려 놓는다.*  
사람: 박스에는 무엇이 담겨있나요?  
로봇: 파란색 피라미드와 파란 블록.  
사람: 피라미드는 무엇으로 지지되고 있나요?  
*앞에서 "어느 피라미드를 의미하는지 몰라요"라고 말하면서 시스템이 "피라미드"에 대해 응답했고, 이제 방금전에 언급한 피라미드를 알고 있다.*  
로봇: 상자.  
사람: 첨탑(steeple) ...  
로봇: 미안합니다. "첨탑(steeple)"이라는 단어를 모르겠어요.  
*시스템이 파악할 수 없는 단어를 보게되면 즉시 시스템이 멈춘다.*  
사람: "첨탑(steeple)"은 스택(stack)인데 두개의 녹색 정육면체와 피라미드를 담고 있다.  
로봇: 이해했습니다.  
사람: 지금 첨탑이 있나요?  
로봇: 아니요.   
*첨탑을 인식하는데 새로운 정의가 설명으로 사용되었다.*  
사람: 그것을 만들어라.  
로봇: OK.  
*정의에 나온 설명이 새로운 물체를 인식하고 구축하는 계획을 추론할만큼 충분하다. 단어 "그것"은 "첨탑"을 의미하고, 계획이 실행된다.*  
~~~
{:class=in}  

다음에 대화가 계속된다&mdash; 실제로 프로그램으로 만들어진 것은 아니지만 확실히 그렇게 할 수도 있다. 마지막 대화에서 새로운 용어(*첨탑(steeple)*)를 정의하는 것이 가능지만, 매우 한정된 용어만 기술할 수 있는데 이유는 로봇이 극단적으로 제한된 "지능"을 소유하기 때문이다.  

~~~
사람: 큰 빨간색 레고블록과 물총을 교환하자.  
로봇: 미안합니다. "교환"이라는 단어를 몰라요.  
사람: "교환"은 재산을 자유로이 바꾸는 것입니다.  
로봇: 미안합니다. "자유로이"라는 단어를 몰라요.  
사람: "자유로이"라는 뜻은 강제없이, 자진하는 행위입니다.   
로봇: 미안합니다. "행위"라는 단어를 몰라요.   
사람: "행위"는 우연히가 아니고 의도적으로 사람이 무언가 한다는 것입니다.  
로봇: 미안합니다. "사람"이라는 단어를 몰라요.  
사람: 미안합니다. 제가 당신이 좀더 똑똑하다고 잘못 생각했네요.  
Robot:  "미안합니다, '미안합니다' 라는 단어 몰라요"  
~~~
{:class=in}  

궁극적으로, 로봇은 자신이 사용한 단어조차도 이해하지 못한다는 사실을 인정해야 한다. ("미안합니다, '미안합니다' 라는 단어 몰라요")!   

이제 교대하고 완전히 가상의 대화를 이어가자. 학생이 대화를 했을 수도 있다. 매우 작지만, 이 작은 대화 파편이 수많은 세계 지식을 시연하고 있다. 

~~~
사람: 큰 빨간 레고블록과 물총을 거래하자.  
로봇? 물총은 이미 있는데, 날씬한 중고 개구리와 레고 블록 2개와 피라미드를 바꿀 수 있어.  
~~~
{:class="out"}  

- 만약 물총을 이미 가지고 있다면, "로봇"은 물총이 덜 가치있다고 평가한다.  
- 더 나아가 *사람이 그 사실을 알기를 기대한다*, 왜냐하면 귀찮게 제안을 거절할 이유를 설명하지 않아도 되기 때문이다.  
- 사람이 뭔가 중요한 것을 넘겨주고서라도 블록을 몹시 원한다는 것을 여전히 로봇이 알고 있다.  
- 또한, 로봇은 물총과는 달리 레고블록 숫자도 중요하다고 생각한다.  
- 개구리를 중고라고 가치를 떨어뜨려 사람을 누그러지게 하려한다.  
- 흥정을 준비한다는 의미를 내포하고 있다.  

확실히, 이런 정도의 인위성은 오늘날 컴퓨터의 능력을 훨씬 뛰어 넘는다.  

다음 스토리는 좀더 최신 컴퓨터 프로그램(1980년대 후반)으로 작성된 것이다. 간결하고 뭔가 부족한 것으로인해 컴퓨터가 생성한 것이라고 즉시 알아채겠지만, 부가적으로 세심함을 일부 더해 꾸미는 것은 어렵지 않게 상상할 수 있다. 흥미로운 점은 스토리의 피상적인 면이 아니라, 스토리가 구현하는 줄거리다. 사람이 쓰는 줄거리와 비교하면 아직 멀었지만, 일부 인간적인 갈등 요소를 잡아내려고 한것처럼 보인다. 요즘에 스토리를 자동으로 만들어내는 시스템이 많이 있다. 하지만, 이런 스토리봇을 평가하는데 도전이 되는 요소는 표준패턴과 차이를 매우는데 얼마나 많은 자료가 필요한지와 창의적으로 줄거리를 얼마나 만들지 결정하는 것이다.  


~~~
옛날에 트루먼이라는 북극 제비갈매기가 있었다. 트루먼은 집이 없었다. 트루먼은 둥지가 필요했다. 해안으로 날아갔다. 나무가지 몇개를 찾아봤다. 나무가지를 찾지 못했다. 툰드라로 날아갔다. 호레이스라는 북극곰을 만났다. 호레이스에게 어디서 나무가지를 찾을 수 있는지 물었다. 호레이스가 나무가지를 숨겼다. 호레이스는 투루먼에게 빙산에 나무가지가 있다고 말했다. 트루먼은 빙산으로 날아갔다. 나무가지를 찾아봤다. 나무가지를 찾을 수 없었다. 호레이스는 고기를 찾아봤다. 고기를 찾았다. 트루먼을 먹었다. 트루먼은 죽었다.  
~~~
{:class="out"}  

매년 *Loebner prize* 행사가 열린다. 심사위원을 컴퓨터가 사람이라고 생각하게 만듦으로서, 튜링 테스트를 통과하려고 컴퓨터 프로그램이 경쟁한다. 2012년 기준, 어떤 컴퓨터도 금상 혹은 은상을 수상하지 못했다. 금상 혹은 은상은 지속적으로  심사위원으로 하여금 컴퓨터가 사람이라고 생각하게 만드는데 성공한 것이다. 하지만, 동상은 매년 수여되는데, 대부분의 심사위원을 속이는데 성공한 컴퓨터에게 돌아간다. 1991년 열린 첫 행사에서 한 프로그램이 간신히 동상을 받았는데 다른 기법중에서 타이핑 실수를 통해서 좀더 인간스럽게 보이는 기법이 수상으로 이어졌다!    

어떤 인공지승 시스템도 완전한 튜링 테스트를 통과할 수 있게 만들어 내지 못했다. 설사 하나가 만들어지더라도, 튜링 테스트를 통과했다고해서 대부분의 사람이 지능이라고 의미하는 것을 진실되게 측정하지 못한다고 많은 철학자가 주장한다. 튜링 테스트가 테스트하는 것은 행위 등가성(behavioral equivalence)이다: 특정 컴퓨터 프로그램이 지능 조짐을 보이는지 판단하도록 디자인되어서 순수하게 지능을 소유했다는 것과 동일하지 않을 수 있다. 인지하지 못하고, 자신을 파악하지 못하고, 자각도 없도, 자의식도 느낄 수 없고, 사랑도 경험하지 못하고, 살아있다고 느끼지도 못하는데, 인간적으로 지능이 있다고 할 수 있을까?  

인공지능(AI) 논쟁은 수십년동안 우리와 함께 할 것 같다.  

##### 추가 읽기  

철학자 John Haugeland가 저술한 *Artificial intelligence: the very idea* 책은 인공지능 논쟁에 대한 매우 잘 저술되어 있고, 이번 활동에 사용된 몇몇 삽화의 출처이기도 하다. (특히, SHRDLU 대화와 대화의 토론)  

최초 튜링 테스트는 앨런 튜링이 1950년 철학저널 *Mind*에 기고한 *"Computing machinery and intelligence"* 에 실렸고, Feigenbaum과 Feldman이 편집한 *Computers and thought*에 다시 출판되었다. 논문에는 첫 두 대화가 실려있다.   
  
심리치료사 프로그램은 1966년 J. Weizenbaum가 *Communications of the Association for Computing Machinery* 컴퓨터 저널 *"ELIZA—A computer program for the study of natural language communication between man and machine"*에 기술되어 있다.  

블록세계(block-world) 로봇 프로그램은 Terry Winograd가 박사학위 논문에 기술되어 있는데, 
*Understanding natural language (Academic Press, New York, 1972)* 책으로도 출판되었다.  

Truman과 Horace 스토리를 만드는 프로그램은 Tony Smith 와 Ian Witten이 1990년 Proceedings of the 10th International Conference on Computing and the Humanities 컨퍼런스에서 *A planning mechanism for generating story text* 제목으로 발표했다. 

</div>  

